{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from knowledge_engine.mpgnn import MPGNN\n",
    "import torch, torch.nn as nn\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.loader import NeighborLoader"
   ],
   "id": "9b70f97855bd53d7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "bf2a0282d8e9fd63"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "genc = MPGNN(nfeat_node=x_dim, nfeat_edge=0, nhid=256, nout=256,\n",
    "             nlayer_gnn=3, node_type='continuous', edge_type='none',\n",
    "             gnn_type='GIN', pooling='mean').cuda()\n",
    "\n",
    "# --- Relation embeddings for typed decoder ---\n",
    "n_rel = num_relation_types  # e.g., is_a, part_of, regulates, etc.\n",
    "rel_emb = nn.Embedding(n_rel, 256).cuda()\n",
    "\n",
    "# DistMult decoder\n",
    "def score(u, r, v):\n",
    "    return (u * r * v).sum(-1)\n",
    "\n",
    "bce = nn.BCEWithLogitsLoss()\n",
    "optim = torch.optim.AdamW(list(genc.parameters()) + list(rel_emb.parameters()), lr=3e-4)\n",
    "\n",
    "# Build PyG Data\n",
    "data = Data(x=node_feats, edge_index=edge_index, edge_type=edge_type)  # edge_type: [E] int64\n",
    "# For huge ontologies, use NeighborLoader to sample subgraphs per step:\n",
    "loader = NeighborLoader(data, num_neighbors=[20, 20], batch_size=4096, input_nodes=None, shuffle=True)\n",
    "\n",
    "temp = 0.2  # InfoNCE temperature\n",
    "def info_nce(anchor, pos, neg):  # all [B, D], neg [B, K, D]\n",
    "    B, D = anchor.size()\n",
    "    pos_sim = torch.cosine_similarity(anchor, pos) / temp  # [B]\n",
    "    neg_sim = (anchor.unsqueeze(1) * neg).sum(-1) / (anchor.norm(dim=-1, keepdim=True) * neg.norm(dim=-1)) / temp  # [B,K]\n",
    "    logits = torch.cat([pos_sim.unsqueeze(1), neg_sim], dim=1)  # [B, 1+K]\n",
    "    labels = torch.zeros(B, dtype=torch.long, device=anchor.device)\n",
    "    return nn.CrossEntropyLoss()(logits, labels)\n",
    "\n",
    "for it, batch in enumerate(loader):\n",
    "    batch = batch.to('cuda')\n",
    "    # 1) Node embeddings on sampled subgraph\n",
    "    H = genc.gnns(genc.input_encoder(batch.x), batch.edge_index, edge_attr=torch.zeros(batch.edge_index.size(1), 1, device='cuda'))\n",
    "    # H: [N_sub, 256]; we won't pool (we need per-node)\n",
    "\n",
    "    # 2) Gather positive edges in the sampled subgraph\n",
    "    ei = batch.edge_index  # [2, E_sub]\n",
    "    et = batch.edge_type   # [E_sub]\n",
    "    u, v = ei[0], ei[1]    # child,parent or subject,object\n",
    "    r = rel_emb(et)\n",
    "\n",
    "    # 3) Negative sampling (corrupt tail)\n",
    "    v_neg = torch.randint_like(v, high=H.size(0))\n",
    "    # (optionally avoid sampling true neighbors)\n",
    "\n",
    "    # 4) DistMult LP loss\n",
    "    pos_s = score(H[u], r, H[v])\n",
    "    neg_s = score(H[u], r, H[v_neg])\n",
    "    y_pos = torch.ones_like(pos_s)\n",
    "    y_neg = torch.zeros_like(neg_s)\n",
    "    L_link = bce(torch.cat([pos_s, neg_s], 0), torch.cat([y_pos, y_neg], 0))\n",
    "\n",
    "    # 5) Hierarchical contrastive (use parent as positive; random non-ancestors as negatives)\n",
    "    # Build a small negative set per u (sample K random nodes)\n",
    "    K = 5\n",
    "    neg_idx = torch.randint(0, H.size(0), (u.size(0), K), device=H.device)\n",
    "    L_contrast = info_nce(H[u], H[v], H[neg_idx])  # cosine InfoNCE\n",
    "\n",
    "    L = L_link + 0.2 * L_contrast\n",
    "    optim.zero_grad(set_to_none=True)\n",
    "    L.backward()\n",
    "    optim.step()\n",
    "\n",
    "# After training, run a full forward once to export:\n",
    "with torch.no_grad():\n",
    "    H_full = genc(data.to('cuda'))  # if MPGNN returns pooled graph, expose an encoder that returns per-node\n",
    "    # If your MPGNN pools, add a method to return node embeddings before pooling.\n",
    "torch.save(H_full.detach().cpu(), \"ontology_embeddings.pt\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
